import numpy as np
import matplotlib.pyplot as plt
import copy
import math
from tqdm import tqdm


class Environment33to3():  # 3x3 í™˜ê²½ ìŠ¹ë¦¬ì¡°ê±´ 3ê°œ

    def __init__(self):
        # ë³´ë“œëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ 9ê°œì˜ ë°°ì—´ë¡œ ì¤€ë¹„
        # ê²Œì„ì¢…ë£Œ : done = True
        self.board_a = np.zeros(9)
        self.done = False
        self.reward = 0
        self.winner = 0
        self.print = False

    def move(self, p1, p2, player):
        # ê° í”Œë ˆì´ì–´ê°€ ì„ íƒí•œ í–‰ë™ì„ í‘œì‹œ í•˜ê³  ê²Œì„ ìƒíƒœ(ì§„í–‰ ë˜ëŠ” ì¢…ë£Œ)ë¥¼ íŒë‹¨
        # p1 = 1, p2 = -1ë¡œ ì •ì˜
        # ê° í”Œë ˆì´ì–´ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” select_action ë©”ì„œë“œë¥¼ ê°€ì§
        if player == 1:
            pos = p1.select_action(env, player)
        else:
            pos = p2.select_action(env, player)

        # ë³´ë“œì— í”Œë ˆì´ì–´ì˜ ì„ íƒì„ í‘œì‹œ
        self.board_a[pos] = player
        if self.print:
            print(player)
            self.print_board()
        # ê²Œì„ì´ ì¢…ë£Œìƒíƒœì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨
        self.end_check(player)

        return self.reward, self.done

    # í˜„ì¬ ë³´ë“œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™(ë‘˜ ìˆ˜ ìˆëŠ” ì¥ì†Œ)ì„ íƒìƒ‰í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    def get_action(self):
        observation = []
        for i in range(9):
            if self.board_a[i] == 0:
                observation.append(i)
        return observation

    # ê²Œì„ì´ ì¢…ë£Œ(ìŠ¹íŒ¨ ë˜ëŠ” ë¹„ê¹€)ëëŠ”ì§€ íŒë‹¨
    def end_check(self, player):
        # 0 1 2
        # 3 4 5
        # 6 7 8
        # ìŠ¹íŒ¨ ì¡°ê±´ì€ ê°€ë¡œ, ì„¸ë¡œ, ëŒ€ê°ì„  ì´ -1 ì´ë‚˜ 1 ë¡œ ë™ì¼í•  ë•Œ
        end_condition = ((0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6))
        for line in end_condition:
            if self.board_a[line[0]] == self.board_a[line[1]] \
                    and self.board_a[line[1]] == self.board_a[line[2]] \
                    and self.board_a[line[0]] != 0:
                # ì¢…ë£Œëë‹¤ë©´ ëˆ„ê°€ ì´ê²¼ëŠ”ì§€ í‘œì‹œ
                self.done = True
                self.reward = player
                return
        # ë¹„ê¸´ ìƒíƒœëŠ” ë”ëŠ” ë³´ë“œì— ë¹ˆ ê³µê°„ì´ ì—†ì„ë•Œ
        observation = self.get_action()
        if (len(observation)) == 0:
            self.done = True
            self.reward = 0
        return

    # í˜„ì¬ ë³´ë“œì˜ ìƒíƒœë¥¼ í‘œì‹œ p1 = O, p2 = X
    def print_board(self):
        print("+----+----+----+")
        for i in range(3):
            for j in range(3):
                if self.board_a[3 * i + j] == 1:
                    print("|  O", end=" ")
                elif self.board_a[3 * i + j] == -1:
                    print("|  X", end=" ")
                else:
                    print("|   ", end=" ")
            print("|")
            print("+----+----+----+")

class Environment44to4():  # 4x4í™˜ê²½ ìŠ¹ë¦¬ì¡°ê±´ 4ê°œ

    def __init__(self):
        # ë³´ë“œëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ 25ê°œì˜ ë°°ì—´ë¡œ ì¤€ë¹„
        # ê²Œì„ì¢…ë£Œ : done = True
        self.board_a = np.zeros(16)
        self.done = False
        self.reward = 0
        self.winner = 0
        self.print = False


    def move(self, p1, p2, player):
        # ê° í”Œë ˆì´ì–´ê°€ ì„ íƒí•œ í–‰ë™ì„ í‘œì‹œ í•˜ê³  ê²Œì„ ìƒíƒœ(ì§„í–‰ ë˜ëŠ” ì¢…ë£Œ)ë¥¼ íŒë‹¨
        # p1 = 1, p2 = -1ë¡œ ì •ì˜
        # ê° í”Œë ˆì´ì–´ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” select_action ë©”ì„œë“œë¥¼ ê°€ì§
        if player == 1:
            pos = p1.select_action(env, player)
        else:
            pos = p2.select_action(env, player)

        # ë³´ë“œì— í”Œë ˆì´ì–´ì˜ ì„ íƒì„ í‘œì‹œ
        self.board_a[pos] = player
        if self.print:
            print(player)
            self.print_board()
        # ê²Œì„ì´ ì¢…ë£Œìƒíƒœì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨
        self.end_check(player)

        return self.reward, self.done

    # í˜„ì¬ ë³´ë“œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™(ë‘˜ ìˆ˜ ìˆëŠ” ì¥ì†Œ)ì„ íƒìƒ‰í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    def get_action(self):
        observation = []
        for i in range(16):
            if self.board_a[i] == 0:
                observation.append(i)
        return observation

    # ê²Œì„ì´ ì¢…ë£Œ(ìŠ¹íŒ¨ ë˜ëŠ” ë¹„ê¹€)ëëŠ”ì§€ íŒë‹¨
    def end_check(self, player):
        # 0 1 2
        # 3 4 5
        # 6 7 8
        # ìŠ¹íŒ¨ ì¡°ê±´ì€ ê°€ë¡œ, ì„¸ë¡œ, ëŒ€ê°ì„  ì´ -1 ì´ë‚˜ 1 ë¡œ ë™ì¼í•  ë•Œ
        end_condition = ((0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14, 15),
                         (0, 4, 8, 12), (1, 5, 9, 13), (2, 6, 10, 14), (3, 7, 11, 15),
                         (0, 5, 10, 15), (3, 6, 9, 12))
        for line in end_condition:
            if self.board_a[line[0]] == self.board_a[line[1]] \
                    and self.board_a[line[1]] == self.board_a[line[2]] \
                    and self.board_a[line[2]] == self.board_a[line[3]] \
                    and self.board_a[line[0]] != 0:
                # ì¢…ë£Œëë‹¤ë©´ ëˆ„ê°€ ì´ê²¼ëŠ”ì§€ í‘œì‹œ
                self.done = True
                self.reward = player
                return
        # ë¹„ê¸´ ìƒíƒœëŠ” ë”ëŠ” ë³´ë“œì— ë¹ˆ ê³µê°„ì´ ì—†ì„ë•Œ
        observation = self.get_action()
        if (len(observation)) == 0:
            self.done = True
            self.reward = 0
        return

    # í˜„ì¬ ë³´ë“œì˜ ìƒíƒœë¥¼ í‘œì‹œ p1 = O, p2 = X
    def print_board(self):
        print("+----+----+----+----+")
        for i in range(4):
            for j in range(4):
                if self.board_a[4 * i + j] == 1:
                    print("|  O", end=" ")
                elif self.board_a[4 * i + j] == -1:
                    print("|  X", end=" ")
                else:
                    print("|   ", end=" ")
            print("|")
            print("+----+----+----+----+")

class Environment55to5():  # 5x5í™˜ê²½ ìŠ¹ë¦¬ì¡°ê±´ 5ê°œ

    def __init__(self):
        # ë³´ë“œëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ 25ê°œì˜ ë°°ì—´ë¡œ ì¤€ë¹„
        # ê²Œì„ì¢…ë£Œ : done = True
        self.board_a = np.zeros(25)
        self.done = False
        self.reward = 0
        self.winner = 0
        self.print = False


    def move(self, p1, p2, player):
        # ê° í”Œë ˆì´ì–´ê°€ ì„ íƒí•œ í–‰ë™ì„ í‘œì‹œ í•˜ê³  ê²Œì„ ìƒíƒœ(ì§„í–‰ ë˜ëŠ” ì¢…ë£Œ)ë¥¼ íŒë‹¨
        # p1 = 1, p2 = -1ë¡œ ì •ì˜
        # ê° í”Œë ˆì´ì–´ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” select_action ë©”ì„œë“œë¥¼ ê°€ì§
        if player == 1:
            pos = p1.select_action(env, player)
        else:
            pos = p2.select_action(env, player)

        # ë³´ë“œì— í”Œë ˆì´ì–´ì˜ ì„ íƒì„ í‘œì‹œ
        self.board_a[pos] = player
        if self.print:
            print(player)
            self.print_board()
        # ê²Œì„ì´ ì¢…ë£Œìƒíƒœì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨
        self.end_check(player)

        return self.reward, self.done

    # í˜„ì¬ ë³´ë“œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™(ë‘˜ ìˆ˜ ìˆëŠ” ì¥ì†Œ)ì„ íƒìƒ‰í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    def get_action(self):
        observation = []
        for i in range(25):
            if self.board_a[i] == 0:
                observation.append(i)
        return observation

    # ê²Œì„ì´ ì¢…ë£Œ(ìŠ¹íŒ¨ ë˜ëŠ” ë¹„ê¹€)ëëŠ”ì§€ íŒë‹¨
    def end_check(self, player):
        # 0 1 2
        # 3 4 5
        # 6 7 8
        # ìŠ¹íŒ¨ ì¡°ê±´ì€ ê°€ë¡œ, ì„¸ë¡œ, ëŒ€ê°ì„  ì´ -1 ì´ë‚˜ 1 ë¡œ ë™ì¼í•  ë•Œ
        end_condition = ((0, 1, 2, 3, 4), (5, 6, 7, 8, 9), (10, 11, 12, 13, 14), (15, 16, 17, 18, 19), (20, 21, 22, 23, 24),
                         (0, 5, 10, 15, 20), (1, 6, 11, 16, 21), (2, 7, 12, 17, 22), (3, 8, 13, 18, 23), (4, 9, 14, 19, 24),
                         (0, 6, 12, 18, 24), (4, 8, 12, 16, 20))
        for line in end_condition:
            if self.board_a[line[0]] == self.board_a[line[1]] \
                    and self.board_a[line[1]] == self.board_a[line[2]] \
                    and self.board_a[line[2]] == self.board_a[line[3]] \
                    and self.board_a[line[3]] == self.board_a[line[4]] \
                    and self.board_a[line[0]] != 0:
                # ì¢…ë£Œëë‹¤ë©´ ëˆ„ê°€ ì´ê²¼ëŠ”ì§€ í‘œì‹œ
                self.done = True
                self.reward = player
                return
        # ë¹„ê¸´ ìƒíƒœëŠ” ë”ëŠ” ë³´ë“œì— ë¹ˆ ê³µê°„ì´ ì—†ì„ë•Œ
        observation = self.get_action()
        if (len(observation)) == 0:
            self.done = True
            self.reward = 0
        return

    # í˜„ì¬ ë³´ë“œì˜ ìƒíƒœë¥¼ í‘œì‹œ p1 = O, p2 = X
    def print_board(self):
        print("+----+----+----+----+----+")
        for i in range(5):
            for j in range(5):
                if self.board_a[5 * i + j] == 1:
                    print("|  O", end=" ")
                elif self.board_a[5 * i + j] == -1:
                    print("|  X", end=" ")
                else:
                    print("|   ", end=" ")
            print("|")
            print("+----+----+----+----+----+")

class Environment55to3():  # 5x5í™˜ê²½ ìŠ¹ë¦¬ì¡°ê±´ 3ê°œ

    def __init__(self):
        # ë³´ë“œëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ 25ê°œì˜ ë°°ì—´ë¡œ ì¤€ë¹„
        # ê²Œì„ì¢…ë£Œ : done = True
        self.board_a = np.zeros(25)
        self.done = False
        self.reward = 0
        self.winner = 0
        self.print = False


    def move(self, p1, p2, player):
        # ê° í”Œë ˆì´ì–´ê°€ ì„ íƒí•œ í–‰ë™ì„ í‘œì‹œ í•˜ê³  ê²Œì„ ìƒíƒœ(ì§„í–‰ ë˜ëŠ” ì¢…ë£Œ)ë¥¼ íŒë‹¨
        # p1 = 1, p2 = -1ë¡œ ì •ì˜
        # ê° í”Œë ˆì´ì–´ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” select_action ë©”ì„œë“œë¥¼ ê°€ì§
        if player == 1:
            pos = p1.select_action(env, player)
        else:
            pos = p2.select_action(env, player)

        # ë³´ë“œì— í”Œë ˆì´ì–´ì˜ ì„ íƒì„ í‘œì‹œ
        self.board_a[pos] = player
        if self.print:
            print(player)
            self.print_board()
        # ê²Œì„ì´ ì¢…ë£Œìƒíƒœì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨
        self.end_check(player)

        return self.reward, self.done

    # í˜„ì¬ ë³´ë“œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™(ë‘˜ ìˆ˜ ìˆëŠ” ì¥ì†Œ)ì„ íƒìƒ‰í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    def get_action(self):
        observation = []
        for i in range(25):
            if self.board_a[i] == 0:
                observation.append(i)
        return observation

    # ê²Œì„ì´ ì¢…ë£Œ(ìŠ¹íŒ¨ ë˜ëŠ” ë¹„ê¹€)ëëŠ”ì§€ íŒë‹¨
    def end_check(self, player):
        # 0 1 2
        # 3 4 5
        # 6 7 8
        # ìŠ¹íŒ¨ ì¡°ê±´ì€ ê°€ë¡œ, ì„¸ë¡œ, ëŒ€ê°ì„  ì´ -1 ì´ë‚˜ 1 ë¡œ ë™ì¼í•  ë•Œ
        end_condition = ((0, 1, 2), (1, 2, 3), (2, 3, 4), (5, 6, 7), (6, 7, 8), (7, 8, 9), (10, 11, 12), (11, 12, 13), (12, 13, 14), (15, 16, 17), (16, 17, 18), (17, 18, 19), (20, 21, 22), (21, 22, 23), (22, 23, 24),
                         (0, 6, 12), (6, 12, 18), (12, 18, 24), (5, 11, 17), (11, 17, 23), (10, 16, 22), (1, 7, 13), (7, 13, 19), (2, 8, 14),
                         (4, 8, 12), (8, 12, 16), (12, 16, 20), (3, 7, 11), (7, 11, 15), (2, 6, 10), (9, 13, 17), (13, 17, 21), (14, 18, 22),
                         (0, 5, 10), (5, 10, 15), (10, 15, 20), (1, 6, 11), (6, 11, 16), (11, 16, 21), (2, 7, 12), (7, 12, 17), (12, 17, 22), (3, 8, 13), (8, 13, 18), (13, 18, 23), (4, 9, 14), (9, 14, 19), (14, 19, 24))
        for line in end_condition:
            if self.board_a[line[0]] == self.board_a[line[1]] \
                    and self.board_a[line[1]] == self.board_a[line[2]] \
                    and self.board_a[line[0]] != 0:
                # ì¢…ë£Œëë‹¤ë©´ ëˆ„ê°€ ì´ê²¼ëŠ”ì§€ í‘œì‹œ
                self.done = True
                self.reward = player
                return
        # ë¹„ê¸´ ìƒíƒœëŠ” ë”ëŠ” ë³´ë“œì— ë¹ˆ ê³µê°„ì´ ì—†ì„ë•Œ
        observation = self.get_action()
        if (len(observation)) == 0:
            self.done = True
            self.reward = 0
        return

    # í˜„ì¬ ë³´ë“œì˜ ìƒíƒœë¥¼ í‘œì‹œ p1 = O, p2 = X
    def print_board(self):
        print("+----+----+----+----+----+")
        for i in range(5):
            for j in range(5):
                if self.board_a[5 * i + j] == 1:
                    print("|  O", end=" ")
                elif self.board_a[5 * i + j] == -1:
                    print("|  X", end=" ")
                else:
                    print("|   ", end=" ")
            print("|")
            print("+----+----+----+----+----+")

class Random_player():

    def __init__(self):
        self.name = "Random player"
        self.print = False

    def select_action(self, env, player):
        # ê°€ëŠ¥í•œ í–‰ë™ ì¡°ì‚¬
        available_action = env.get_action()
        # ê°€ëŠ¥í•œ í–‰ë™ ì¤‘ í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ
        action = np.random.randint(len(available_action))
        #         print("Select action(random) = {}".format(available_action[action]))
        return available_action[action]

    class Monte_Carlo_player():

        def __init__(self):
            self.name = "MC player"
            self.num_playout = 1000

        def select_action(self, env, player):
            # ê°€ëŠ¥í•œ í–‰ë™ ì¡°ì‚¬
            available_action = env.get_action()
            V = np.zeros(len(available_action))

            for i in range(len(available_action)):
                # í”Œë ˆì´ì•„ì›ƒì„ 1000ë²ˆ ë°˜ë³µ
                for j in range(self.num_playout):
                    # ì§€ê¸ˆ ìƒíƒœë¥¼ ë³µì‚¬í•´ì„œ í”Œë ˆì´ ì•„ì›ƒì— ì‚¬ìš©
                    temp_env = copy.deepcopy(env)
                    # í”Œë ˆì´ì•„ì›ƒì˜ ê²°ê³¼ëŠ” ìŠ¹ë¦¬ í”Œë ˆì´ì–´ì˜ ê°’ìœ¼ë¡œ ë°˜í™˜
                    # p1 ì´ ì´ê¸°ë©´ 1, p2 ê°€ ì´ê¸°ë©´ -1
                    self.playout(temp_env, available_action[i], player)
                    if player == temp_env.reward:
                        V[i] += 1

            return available_action[np.argmax(V)]

            # í”Œë ˆì´ì•„ì›ƒ ì¬ê·€í•¨ìˆ˜

        # ê²Œì„ì´ ì¢…ë£Œìƒíƒœ (ìŠ¹ ë˜ëŠ” íŒ¨ ë˜ëŠ” ë¹„ê¹€) ê°€ ë ë•Œê¹Œì§€ í–‰ë™ì„ ì„ì˜ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì„ ë°˜ë³µ
        # í”Œë ˆì´ì–´ëŠ” ê³„ì† ë°”ë€Œê¸° ë•Œë¬¸ì— (-)ë¥¼ ê³±í•´ì„œ -1, 1, -1 ì´ ë˜ê²Œí•¨
        def playout(self, temp_env, action, player):

            temp_env.board_a[action] = player
            temp_env.end_check(player)
            # ê²Œì„ ì¢…ë£Œ ì²´í¬
            if temp_env.done == True:
                return
            else:
                # í”Œë ˆì´ì–´ êµì²´
                player = -player
                # ê°€ëŠ¥í•œ í–‰ë™ ì¡°ì‚¬
                available_action = temp_env.get_action()
                # ë¬´ì‘ìœ„ë¡œ í–‰ë™ì„ ì„ íƒ
                action = np.random.randint(len(available_action))
                self.playout(temp_env, available_action[action], player)


class MG_player():
    def __init__(self):
        self.name = "MG_player"
        self.v_table = np.zeros((3, 3, 3, 3, 3, 3, 3, 3, 3, 9))
        self.Itr_Policy(self.v_table)
        self.print = False

    def select_action(self, env, player):
        a = (int)(env.board_a[0])
        b = (int)(env.board_a[1])
        c = (int)(env.board_a[2])
        d = (int)(env.board_a[3])
        e = (int)(env.board_a[4])
        f = (int)(env.board_a[5])
        g = (int)(env.board_a[6])
        h = (int)(env.board_a[7])
        i = (int)(env.board_a[8])

        available_action = env.get_action()

        # v_tableì—ì„œ í˜„ì¬ ì •ë³´ì— ê°€ì¥ ê°’ì´ ë†’ì€ actionìœ¼ë¡œ ê²°ì •
        max_Value = self.v_table[a][b][c][d][e][f][g][h][i][available_action[-1]]
        action = available_action[-1]

        if self.print:
            print("{} : select action".format(action))

        for j in available_action:
            if self.v_table[a][b][c][d][e][f][g][h][i][j] > max_Value:
                action = j
                max_Value = self.v_table[a][b][c][d][e][f][g][h][i][j]
        return action

    def Itr_Policy(self, v_table):
        env = Environment44to4() # ê²Œì„ í™˜ê²½ ì§€ì •
        gamma = 0.9
        k = 0

        while True:
            k += 1
            # Î”â†0
            delta = 0
            # ê³„ì‚°ì „ ê°€ì¹˜ë¥¼ ì €ì¥
            temp_v = copy.deepcopy(v_table)
            for a in [-1, 0, 1]:
                for b in [-1, 0, 1]:
                    for c in [-1, 0, 1]:
                        for d in [-1, 0, 1]:
                            for e in [-1, 0, 1]:
                                for f in [-1, 0, 1]:
                                    for g in [-1, 0, 1]:
                                        for h in [-1, 0, 1]:
                                            for i in [-1, 0, 1]:
                                                for action in range(9):
                                                    G = 0
                                                    env.reward = 0
                                                    # ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´
                                                    env.board_a = np.asarray([a, b, c, d, e, f, g, h, i])
                                                    # actionìœ¼ë¡œ ì¸í•œ ì´ë™ì— ëŒ€í•œ ê³„ì‚°
                                                    if action == 0 and env.board_a[0] == 0:
                                                        env.board_a[0] = 1
                                                        env.reward -= 1
                                                    elif action == 1 and env.board_a[1] == 0:
                                                        env.board_a[1] = 1
                                                        env.reward -= 1
                                                    elif action == 2 and env.board_a[2] == 0:
                                                        env.board_a[2] = 1
                                                        env.reward -= 1
                                                    elif action == 3 and env.board_a[3] == 0:
                                                        env.board_a[3] = 1
                                                        env.reward -= 1
                                                    elif action == 4 and env.board_a[4] == 0:
                                                        env.board_a[4] = 1
                                                        env.reward -= 0.5
                                                    elif action == 5 and env.board_a[5] == 0:
                                                        env.board_a[5] = 1
                                                        env.reward -= 1
                                                    elif action == 6 and env.board_a[6] == 0:
                                                        env.board_a[6] = 1
                                                        env.reward -= 1
                                                    elif action == 7 and env.board_a[7] == 0:
                                                        env.board_a[7] = 1
                                                        env.reward -= 1
                                                    elif action == 8 and env.board_a[8] == 0:
                                                        env.board_a[8] = 1
                                                        env.reward -= 1
                                                    else:
                                                        # ì´ë¯¸ ë‘” ê³µê°„ì— ë‘ëŠ” ê²½ìš°
                                                        env.reward -= 5

                                                    end_condition = (
                                                    (0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8),
                                                    (0, 4, 8), (2, 4, 6))
                                                    for line in end_condition:
                                                        # ì´ê¸¸ ê²½ìš°
                                                        if env.board_a[line[0]] == 1 and env.board_a[line[0]] == \
                                                                env.board_a[line[1]] and env.board_a[line[1]] == \
                                                                env.board_a[line[2]] and env.board_a[line[0]] != 0:
                                                            env.reward += 3
                                                        # ì§ˆ ê²½ìš°
                                                        if env.board_a[line[0]] == -1 and env.board_a[line[0]] == \
                                                                env.board_a[line[1]] and env.board_a[line[1]] == \
                                                                env.board_a[line[2]] and env.board_a[line[0]] != 0:
                                                            env.reward -= 3

                                                    # ë¹„ê¸¸ ê²½ìš°
                                                    observation = env.get_action()
                                                    if (len(observation)) == 0:
                                                        env.reward -= 2

                                                    # ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ìœ¼ë¡œ ë‹¤ìŒìƒíƒœë§Œ ì´ìš©í•´ ğ‘‰(ğ‘ ) ê³„ì‚°
                                                    G += (1 / 9) * (env.reward + gamma * v_table[
                                                        a, b, c, d, e, f, g, h, i, action])
                                                    v_table[a, b, c, d, e, f, g, h, i, action] = G

            delta = np.max([delta, np.max(np.abs(temp_v - v_table))])
            print("Iteration " + (str)(k) + " delta : " + (str)(delta))
            if delta < 0.000001:
                break
        print("Iteration Complete")


class KMGPlayer():

    def __init__(self):
        self.name = "KMG"
        self.num_play = 100

    def select_action(self, env, player):
        available_action = env.get_action()
        V = np.zeros(len(available_action))

        for i in range(len(available_action)):
            for j in range(self.num_play):
                temp_env = copy.deepcopy(env)
                self.play(temp_env, available_action[i], player)
                if len(available_action) == 25:
                    V[12] = 120

                elif player == temp_env.reward:
                    V[i] += 1

        return available_action[np.argmax(V)]

    def play(self, temp_env, action, player):
        temp_env.board_a[action] = player
        temp_env.end_check(player)

        if temp_env.done == True:
            return

        else:
            player = -player
            available_action = temp_env.get_action()
            action = np.random.randint(len(available_action))
            self.play(temp_env, available_action[action], player)

np.random.seed(0)

# p1 = KMG_player()
# p2 = MG_player()

# p1 = Human_player()
# p2 = Human_player()

# p1 = Random_player()
p2 = Random_player()

p1 = KMGPlayer()
# p1.num_playout = 100
# p2 = Monte_Carlo_player()
# p2.num_playout = 1000

# p1 = p1_Qplayer
# p1.epsilon = 0

# p2 = p2_Qplayer
# p2.epsilon = 0

# p1 = p1_DQN
# p1.epsilon = 0

# ì§€ì •ëœ ê²Œì„ ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ë‘ê²Œ í•  ê²ƒì¸ì§€ í•œê²Œì„ì”© ë‘ê²Œ í•  ê²ƒì¸ì§€ ê²°ì •
# auto = True : ì§€ì •ëœ íŒìˆ˜(games)ë¥¼ ìë™ìœ¼ë¡œ ì§„í–‰
# auto = False : í•œíŒì”© ì§„í–‰

auto = False

# auto ëª¨ë“œì˜ ê²Œì„ìˆ˜
games = 100

print("pl player : {}".format(p1.name))
print("p2 player : {}".format(p2.name))

# ê° í”Œë ˆì´ì–´ì˜ ìŠ¹ë¦¬ íšŸìˆ˜ë¥¼ ì €ì¥
p1_score = 0
p2_score = 0
draw_score = 0

if auto:
    # ìë™ ëª¨ë“œ ì‹¤í–‰
    for j in tqdm(range(games)):

        np.random.seed(j)
        env = Environment55to5()  # ê²Œì„ í™˜ê²½ ì§€ì •

        for i in range(10000):
            # p1 ê³¼ p2ê°€ ë²ˆê°ˆì•„ ê°€ë©´ì„œ ê²Œì„ì„ ì§„í–‰
            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...
            reward, done = env.move(p1, p2, (-1) ** i)
            # ê²Œì„ ì¢…ë£Œ ì²´í¬
            if done == True:
                if reward == 1:
                    p1_score += 1
                elif reward == -1:
                    p2_score += 1
                else:
                    draw_score += 1
                break

else:
    # í•œ ê²Œì„ì”© ì§„í–‰í•˜ëŠ” ìˆ˜ë™ ëª¨ë“œ
    np.random.seed(1)
    while True:

        env = Environment55to5() # ê²Œì„ í™˜ê²½ ì§€ì •
        env.print = False
        for i in range(10000):
            reward, done = env.move(p1, p2, (-1) ** i)
            env.print_board()
            if done == True:
                if reward == 1:
                    print("winner is p1({})".format(p1.name))
                    p1_score += 1
                elif reward == -1:
                    print("winner is p2({})".format(p2.name))
                    p2_score += 1
                else:
                    print("draw")
                    draw_score += 1
                break

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("final result")
        env.print_board()

        # í•œê²Œì„ ë”?ìµœì¢… ê²°ê³¼ ì¶œë ¥
        answer = input("More Game? (y/n)")

        if answer == 'n':
            break

print("p1({}) = {} p2({}) = {} draw = {}".format(p1.name, p1_score, p2.name, p2_score, draw_score))
